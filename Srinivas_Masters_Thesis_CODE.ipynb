{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/STRMNGnwo/Masters-Thesis/blob/main/Srinivas_Masters_Thesis_CODE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6wDDa_K-Yjt"
      },
      "source": [
        "# Installs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6iAo23uBW-SZ"
      },
      "source": [
        "### Added code to use specific versions of keras and tensorflow until the open issue on the keras stable diffusion implementation is resolved"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EPKuVKZjXJpR"
      },
      "outputs": [],
      "source": [
        "# Uninstall existing versions\n",
        "!pip uninstall -y keras keras-core keras-cv tensorflow\n",
        "\n",
        "# Install specific versions\n",
        "!pip install keras==2.15.0 keras-core==0.1.7 keras-cv==0.9.0 tensorflow==2.15.1\n",
        "\n",
        "# Restart the runtime to ensure the changes take effect\n",
        "import os\n",
        "os.kill(os.getpid(), 9)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a3yg3PoMXLxy"
      },
      "outputs": [],
      "source": [
        "'''import keras\n",
        "import keras_core\n",
        "import keras_cv\n",
        "\n",
        "#print(\"TensorFlow version:\", tf.__version__)\n",
        "print(\"Keras version:\", keras.__version__)\n",
        "print(\"Keras Core version:\", keras_core.__version__)\n",
        "print(\"Keras CV version:\", keras_cv.__version__)'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkmBS1bAaeJD"
      },
      "source": [
        "### The usual NLP plus libraries to access pre-trained models from HuggingFace, the openai api and CLIP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W8VHHjoh-aTY"
      },
      "outputs": [],
      "source": [
        "!pip install nltk\n",
        "#used to perform semantic similarity analysis after evolutionary operators are used (to make sure they make sense)\n",
        "!pip install sentence_transformers\n",
        "!pip install language_tool_python\n",
        "\n",
        "#to use the clip model as a fitness function\n",
        "!pip install git+https://github.com/openai/CLIP.git\n",
        "#to use the openai api to access the gpt 4o model\n",
        "!pip install openai\n",
        "\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7V7NtKd2W80i"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQ05IG3n9UdS"
      },
      "source": [
        "# Imports/ Downloads"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QPXFuCXP8WbQ"
      },
      "outputs": [],
      "source": [
        "#fundamental imports\n",
        "import os\n",
        "import sys\n",
        "import random\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch\n",
        "\n",
        "import nltk\n",
        "from transformers import pipeline\n",
        "import clip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F5i1ryTrgzWt"
      },
      "outputs": [],
      "source": [
        "#downloads that are necessary for the NLP based mutation and cross-over to occur\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "#used to perform pos tagging (used to only mutate adjectives, verbs and nouns in mutation-synonym based and also contextual )\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "\n",
        "#used to obtain synonyms during mutation\n",
        "nltk.download('words')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "#used to ignore/remove stopwords during mutation\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W7uj52db_10G"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "from google.colab import drive\n",
        "\n",
        "\n",
        "drive.mount(\"/content/drive/\")\n",
        "sys.path.insert(0, '/content/drive/MyDrive/Masters-Thesis-Datasets/')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPvDUkWn9k8H"
      },
      "source": [
        "# Checking if GPU is available (and asking Colab to use it if it is)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lDGLZboN9lo7"
      },
      "outputs": [],
      "source": [
        "if torch.cuda.is_available():\n",
        "  DEVICE=torch.device(\"cuda\")\n",
        "  torch.cuda.get_device_name(0)\n",
        "  print(\"GPU\")\n",
        "\n",
        "else:\n",
        "  DEVICE=\"cpu\"\n",
        "  print(\"CPU\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1i4qaFy9XOP"
      },
      "source": [
        "# Importing and initialising a text to image stable diffusion model\n",
        "\n",
        "Also a SpaCy English model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "INvTYQ3j9b1Y"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import keras_cv\n",
        "import keras\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ao9ot28L-XFv"
      },
      "outputs": [],
      "source": [
        "sd_model = keras_cv.models.StableDiffusion(\n",
        "    img_width=256, img_height=256, jit_compile=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dm3zd0yHhiMo"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "\n",
        "# Loading the SpaCy model to perform Named Entity Recognition to get \"descriptive\" words\n",
        "spacy_model = spacy.load(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "If95y2mJ-t7N"
      },
      "source": [
        "# Function to plot images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PfXB8JCm-wNR"
      },
      "outputs": [],
      "source": [
        "def plot_images(images):\n",
        "    plt.figure(figsize=(20, 20))\n",
        "    for i in range(len(images)):\n",
        "        ax = plt.subplot(1, len(images), i + 1)\n",
        "        plt.imshow(images[i])\n",
        "        plt.axis(\"off\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxrABJsa_qd-"
      },
      "source": [
        "# Importing the dataset of Van Gogh paintings\n",
        "\n",
        "These will serve as ground truth reference images and will also be provided to the LLM to get prompts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVupSNCY3cJP"
      },
      "source": [
        "### Unzipping the dataset from drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b5-a3AC3S92x"
      },
      "outputs": [],
      "source": [
        "!unzip /content/drive/MyDrive/Masters-Thesis-Datasets/VanGogh.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMQL2eFP0yXU"
      },
      "source": [
        "### Function to randomly choose an image from the dataset (or choose a specified image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wiFi1JUf03M9"
      },
      "outputs": [],
      "source": [
        "base_url=\"/content/VincentVanGogh/Watercolors\"\n",
        "def choose_image(image_path=None, return_image=True):\n",
        "\n",
        "  #if no image path is specified choose a random image path from the Watercolors directory\n",
        "  if image_path==None:\n",
        "    image_paths = os.listdir(base_url)\n",
        "    print(\"Num images:\",len(image_paths))\n",
        "    random_image_path = random.choice(image_paths)\n",
        "    print(\"Chosen image path is: \",random_image_path)\n",
        "    if return_image:\n",
        "      return Image.open(os.path.join(base_url,random_image_path))\n",
        "    else:\n",
        "      return random_image_path\n",
        "\n",
        "  else:\n",
        "    if return_image:\n",
        "      return Image.open(image_path)\n",
        "    else:\n",
        "      return image_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NG80Gsy8Q-tG"
      },
      "outputs": [],
      "source": [
        "chosen_image=choose_image(image_path=None, return_image=True)\n",
        "print(type(chosen_image))\n",
        "\n",
        "display(chosen_image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6lf03woeMww"
      },
      "source": [
        "# LLM/API sections\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5h015nYeTah"
      },
      "source": [
        "### Establishing a connection to the OpenAI API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "axTz2XLSeY3j"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "secret_key = userdata.get('OPENAI_APIKEY')\n",
        "\n",
        "# OpenAI API Key\n",
        "api_key = secret_key\n",
        "\n",
        "import openai\n",
        "\n",
        "client = openai.OpenAI(api_key=api_key)\n",
        "#gpt-3.5-turbo-instruct has a price of US$0.50 /1M INPUT tokens and US$2.00 / 1M OUTPUT tokens\n",
        "\n",
        "#PROBABLY BEST TO USE THE BELOW MODELS FOR TASKS: correcting grammar & sentences, providing image and getting captions back\n",
        "\n",
        "#gpt-4o-mini has a price of US$0.150 / 1M input tokens AND US$0.600 / 1M OUTPUT tokens\n",
        "\n",
        "#gpt-4o-mini-2024-07-18 has a price of US$0.150 / 1M input tokens AND US$0.600 / 1M OUTPUT tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sz21Mgg4mjmV"
      },
      "source": [
        "### Function to send chosen image from dataset to LLM (via OpenAI API) and get list of descriptions (these will be used as initial population)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZsLwgwysmt0N"
      },
      "outputs": [],
      "source": [
        "import base64\n",
        "import requests\n",
        "\n",
        "# Function to encode the image\n",
        "def encode_image(image_path):\n",
        "  with open(image_path, \"rb\") as image_file:\n",
        "    return base64.b64encode(image_file.read()).decode('utf-8')\n",
        "\n",
        "def get_initial_descriptions(path_to_image,num_descriptions):\n",
        "\n",
        "  BASE_PROMPT= f\"Could you give me {num_descriptions} alt-text descriptions of the attached image, with each description describing the image?. Each description should vary in complexity (some like a 10-year old  wrote it, some like a professional artist wrote it, some like a writer wrote it etcetera) and length.\"\n",
        "  image_path = path_to_image\n",
        "\n",
        "  # Getting the base64 string\n",
        "  base64_image = encode_image(image_path)\n",
        "\n",
        "  headers = {\n",
        "    \"Content-Type\": \"application/json\",\n",
        "    \"Authorization\": f\"Bearer {api_key}\"\n",
        "  }\n",
        "\n",
        "  payload = {\n",
        "    \"model\": \"gpt-4o-mini\",\n",
        "    \"messages\": [\n",
        "      {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "          {\n",
        "            \"type\": \"text\",\n",
        "            \"text\": BASE_PROMPT\n",
        "          },\n",
        "          {\n",
        "            \"type\": \"image_url\",\n",
        "            \"image_url\": {\n",
        "              \"url\": f\"data:image/jpeg;base64,{base64_image}\"\n",
        "            }\n",
        "          }\n",
        "        ]\n",
        "      }\n",
        "    ],\n",
        "    \"max_tokens\": 800\n",
        "  }\n",
        "\n",
        "  response = requests.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload)\n",
        "\n",
        "  print(response.json())\n",
        "\n",
        "  return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j7Od5enXQHXI"
      },
      "outputs": [],
      "source": [
        "#descriptions_response=get_initial_descriptions(path_to_image=\"/content/VincentVanGogh/Watercolors/The Langlois Bridge at Arles.jpg\",num_descriptions=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oY1V1Qo6b2Cw"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from copy import deepcopy\n",
        "\n",
        "'''\n",
        "#descriptions_response_copy=deepcopy(descriptions_response)\n",
        "\n",
        "desc_resp_json=descriptions_response.json()\n",
        "#desc_resp_json[\"choices\"][0]['message']['content']\n",
        "\n",
        "# Regular expression pattern to extract descriptions, ignoring the initial text\n",
        "pattern = re.compile(r'\\d+\\.\\s+\\*\\*.*?\\*\\*:\\s*(.*?)(?=\\n\\d+\\.\\s|\\Z)', re.DOTALL)\n",
        "\n",
        "# Remove the introductory part\n",
        "intro_removed_text = re.sub(r'^Here are \\d+ different alt-text descriptions for the image.*?\\n\\n', '', desc_resp_json[\"choices\"][0]['message']['content'], flags=re.DOTALL).strip()\n",
        "\n",
        "# Find all descriptions\n",
        "descriptions = pattern.findall(intro_removed_text)\n",
        "\n",
        "# Print the list of descriptions\n",
        "for i, desc in enumerate(descriptions, 1):\n",
        "    print(f\"{desc.strip()}\")\n",
        "\n",
        "'''\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00f5ttOtIVqK"
      },
      "source": [
        "### Hugging Face Instruct model - locally\n",
        "\n",
        "Currently used for sentence correction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H6GlHy265GWZ"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "device = \"cuda\" # the device to load the model onto\n",
        "\n",
        "#\"microsoft/Phi-3-mini-4k-instruct\"\n",
        "#\"Qwen/Qwen2-7B-Instruct\"\n",
        "llm_model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"microsoft/Phi-3-mini-4k-instruct\",\n",
        "    torch_dtype=\"auto\",\n",
        "    device_map=\"auto\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wCZ78_UEGJhZ"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=llm_model,\n",
        "    tokenizer=tokenizer,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3u8p5YjCILGO"
      },
      "outputs": [],
      "source": [
        "#function that should use the initialised microsoft 3B param instruct model locally to correct sentences (instead of making api calls)\n",
        "def correct_sentences(sentences):\n",
        "  child_sentences=[]\n",
        "  for sentence in sentences:\n",
        "    #sentence=\"Vibrant boats an orange beach a vivid blue sky their bold colors the ocean.\"\n",
        "    prompt = f\"Correct this sentence: {sentence} and return only the corrected sentence.\"\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": prompt}\n",
        "    ]\n",
        "\n",
        "    generation_args = {\n",
        "        \"max_new_tokens\": 500,\n",
        "        \"return_full_text\": False,\n",
        "        \"temperature\": 0.0,\n",
        "        \"do_sample\": False,\n",
        "    }\n",
        "\n",
        "    output = pipe(messages, **generation_args)\n",
        "    child_sentences.append(output[0]['generated_text'])\n",
        "\n",
        "  return child_sentences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDDoBjjuIhM6"
      },
      "source": [
        "### Grammar/sentence correction function- OpenAI API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0WsrRAjigSXQ"
      },
      "outputs": [],
      "source": [
        "def correct_sentences_api(sentences):\n",
        "  sentence1,*sentence2=sentences\n",
        "  response = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini-2024-07-18\",\n",
        "        messages=[\n",
        "        {\"role\": \"user\", \"content\": f\"Correct the following sentences so they become standard English and return each corrected sentence separated by a delimiter '|':\\n1. '{sentence1}'\\n2. '{sentence2}'\"}\n",
        "        ],\n",
        "        max_tokens=len(sentence1) + len(sentence2) + 77,  # Ensure enough tokens for the response\n",
        "        n=1,\n",
        "        stop=None,\n",
        "        temperature=0.5,\n",
        "    )\n",
        "\n",
        "  corrected_sentences = response.choices[0].message.content\n",
        "  return corrected_sentences.split(\"|\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mb4xATfjbBqm"
      },
      "source": [
        "### Function to get \"Descriptive words\" about the chosen image.\n",
        "\n",
        "These words can be used in a hopefully hugely beneficial mutation function (this should not be triggered often) that can append words like \"painting, Van Gogh\" etcetera to the end of a caption to make it more descriptive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uvHhdYWbcDov"
      },
      "outputs": [],
      "source": [
        "def get_descriptive_words_api(path_to_image, num_words):\n",
        "  BASE_PROMPT= f\"Generate {num_words} descriptive words or phrases for this image.\"\n",
        "  get_descriptives_prompt= f\"Could you give me {num_words} words that are descriptive of this image?. Examples of such words are painting, watercolours\"\n",
        "  image_path = path_to_image\n",
        "\n",
        "  # Getting the base64 string\n",
        "  base64_image = encode_image(image_path)\n",
        "\n",
        "  headers = {\n",
        "    \"Content-Type\": \"application/json\",\n",
        "    \"Authorization\": f\"Bearer {api_key}\"\n",
        "  }\n",
        "\n",
        "  payload = {\n",
        "    \"model\": \"gpt-4o-mini\",\n",
        "    \"messages\": [\n",
        "      {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "          {\n",
        "            \"type\": \"text\",\n",
        "            \"text\": BASE_PROMPT\n",
        "          },\n",
        "          {\n",
        "            \"type\": \"image_url\",\n",
        "            \"image_url\": {\n",
        "              \"url\": f\"data:image/jpeg;base64,{base64_image}\"\n",
        "            }\n",
        "          }\n",
        "        ]\n",
        "      }\n",
        "    ],\n",
        "    \"max_tokens\": 500\n",
        "  }\n",
        "\n",
        "  response = requests.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload)\n",
        "\n",
        "  print(response.json())\n",
        "\n",
        "  resp_words=response.choices[0].message.content\n",
        "\n",
        "  # Split the text into individual lines\n",
        "  lines = resp_words.strip().split(\"\\n\")\n",
        "\n",
        "  # Extract the descriptive words/phrases without the numbers and create a list\n",
        "  descriptive_list = [line.split(\". \", 1)[1] for line in lines]\n",
        "\n",
        "  # Print the list\n",
        "  print(descriptive_list)\n",
        "  return descriptive_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MelTDVW2ofUq"
      },
      "source": [
        "### Function to get descriptive words using NER - Not used yet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZgwkTv8koinn"
      },
      "outputs": [],
      "source": [
        "# List of image descriptions\n",
        "descriptions = [\n",
        "    \"The image shows a beach scene with several colorful fishing boats.\",\n",
        "    \"Fishing boats are resting on the sandy shore under a clear blue sky.\",\n",
        "    \"Brightly painted boats are anchored on the golden beach with the ocean in the background.\",\n",
        "    \"A vivid scene of boats on the shore with the horizon meeting the sea.\",\n",
        "    \"Fishing vessels with colorful hulls on a sunny beach day.\",\n",
        "]\n",
        "\n",
        "\n",
        "# Function to extract unique phrases\n",
        "def extract_unique_phrases(descriptions):\n",
        "  unique_phrases = set()\n",
        "  for description in descriptions:\n",
        "      doc = spacy_model(description)\n",
        "      for chunk in doc.noun_chunks:\n",
        "          unique_phrases.add(chunk.text.lower())\n",
        "  return unique_phrases\n",
        "\n",
        "# Extract unique phrases\n",
        "#unique_phrases = extract_unique_phrases(descriptions)\n",
        "\n",
        "# Print the unique phrases\n",
        "#print(unique_phrases)\n",
        "\n",
        "'''\n",
        "# Extract unique entities\n",
        "unique_entities = extract_unique_entities(descriptions)\n",
        "\n",
        "# Print the unique entities\n",
        "print(unique_entities)\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "quj8vdr-l9GT"
      },
      "source": [
        "### Testing the LLM that is accessed via API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iyxaciKiFB1p"
      },
      "outputs": [],
      "source": [
        "#incorrect_sentence=\"On a golden beach , where the lazuline sea kisses the boats are on a beach succeeding to the ocean .\"\n",
        "\n",
        "#print(correct_sentences_api(incorrect_sentence))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qa-BrWYyTIw_"
      },
      "source": [
        "# Evolutionary Computing Aspects"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P6Fydre0y7OD"
      },
      "outputs": [],
      "source": [
        "# trial generation of an image from initial population\n",
        "reference_image_path=\"/content/VincentVanGogh/Watercolors/The Night Cafe in Arles.jpg\"\n",
        "\n",
        "reference_image= Image.open(reference_image_path)\n",
        "\n",
        "trial_prompt_1=\"Four colorful boats are on a beach next to the ocean.\"\n",
        "trial_prompt_2=\"Vibrant boats rest on an orange beach under a vivid blue sky, their bold colors creating a striking contrast with the natural surroundings.\"\n",
        "\n",
        "trial_prompt_3=\" Vibrant boats with their bold colors sail on an orange beach under a vivid blue sky.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_kn7T7xgp-c"
      },
      "source": [
        "### Model Trial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DVvKtDfYgtcu"
      },
      "outputs": [],
      "source": [
        "image_1 = sd_model.text_to_image(trial_prompt_2, batch_size=1)\n",
        "plot_images(image_1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uSFdO_cVwkEw"
      },
      "source": [
        "### Helper function to generate images for a prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VenfvxXRwnU8"
      },
      "outputs": [],
      "source": [
        "def generate_image(individual):\n",
        "  generated_image=sd_model.text_to_image(individual[\"prompt\"], batch_size=1)\n",
        "  return generated_image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BgMJeAyhxaJ"
      },
      "source": [
        "### NLP helper functions to aid mutation and cross-over"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cZxgk5Rzh0Kp"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import wordnet,stopwords\n",
        "from nltk import pos_tag, word_tokenize\n",
        "from nltk.chunk import ne_chunk\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "#import language_tool_python\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "#used to check if sentance after mutation and cross-over is grammatically correct and semantically similar\n",
        "#tool = language_tool_python.LanguageTool('en-US')\n",
        "transformer_model = SentenceTransformer('paraphrase-MiniLM-L6-v2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k3LbgQbmiEGz"
      },
      "outputs": [],
      "source": [
        "#used to check whether sentence is semantically similar after synonym mutation\n",
        "def is_sentence_semantically_similar(original_sentence, mutated_sentence, threshold=0.7):\n",
        "    embeddings = transformer_model.encode([original_sentence, mutated_sentence])\n",
        "    similarity = util.pytorch_cos_sim(embeddings[0], embeddings[1]).item()\n",
        "    #print(\"Similarity is: \"+str(similarity))\n",
        "    return similarity >= threshold\n",
        "\n",
        "def get_semantic_similarity(word1, word2):\n",
        "    embeddings = transformer_model.encode([word1, word2])\n",
        "    return util.pytorch_cos_sim(embeddings[0], embeddings[1]).item()\n",
        "\n",
        "#used in the mutation function that is based on synonyms\n",
        "def get_synonyms(word,threshold=0.8):\n",
        "    synonyms = set()\n",
        "    for syn in wordnet.synsets(word):\n",
        "        for lemma in syn.lemmas():\n",
        "            synonyms.add(lemma.name())\n",
        "    synonyms.discard(word)\n",
        "    synonyms_list=list(synonyms)\n",
        "\n",
        "#making sure that only synonyms that are similar in meaning to the word being mutated are returned\n",
        "#boat and gravy boat have a similarity of 0.6, so anything 0.75 and over should be good\n",
        "    filtered_synonyms=[]\n",
        "    for synonym in synonyms_list:\n",
        "      if get_semantic_similarity(word,synonym)>=threshold:\n",
        "        filtered_synonyms.append(synonym)\n",
        "\n",
        "    return filtered_synonyms\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rsBk8z3a_tzg"
      },
      "outputs": [],
      "source": [
        "#utilised in the cross-over function\n",
        "def extract_phrases(sentence):\n",
        "    words = word_tokenize(sentence)\n",
        "    tagged = pos_tag(words)\n",
        "    chunks = ne_chunk(tagged)\n",
        "    phrases = []\n",
        "    current_phrase = []\n",
        "    for chunk in chunks:\n",
        "        if hasattr(chunk, 'label'):\n",
        "            current_phrase.append(' '.join(c[0] for c in chunk))\n",
        "        else:\n",
        "            if current_phrase:\n",
        "                phrases.append(' '.join(current_phrase))\n",
        "                current_phrase = []\n",
        "            phrases.append(chunk[0])\n",
        "    if current_phrase:\n",
        "        phrases.append(' '.join(current_phrase))\n",
        "    return phrases\n",
        "\n",
        "#utilised in the noun phrase based crossover\n",
        "def extract_noun_phrases_spacy(text):\n",
        "    # Process the text with spaCy\n",
        "    doc = spacy_model(text)\n",
        "\n",
        "    # Extract noun phrases\n",
        "    phrases = [chunk.text for chunk in doc.noun_chunks]\n",
        "    return phrases\n",
        "\n",
        "\n",
        "sentence = \"The quick brown fox jumps over the lazy dog near the Eiffel Tower.\"\n",
        "phrases=extract_phrases(sentence)\n",
        "print(phrases)\n",
        "phrases_spacy = extract_noun_phrases_spacy(sentence)\n",
        "print(phrases_spacy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fy-SPTdTTX1c"
      },
      "source": [
        "### Defining the SSIM fitness function to compare generated image with reference image"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reference_image_resized = reference_image.resize((256, 256))\n",
        "reference_image_pil=reference_image_resized\n",
        "reference_image_np=np.asarray(reference_image_pil)"
      ],
      "metadata": {
        "id": "ddBwfaswBUri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gqvaNV7DTc3c"
      },
      "outputs": [],
      "source": [
        "from skimage.metrics import structural_similarity as ssim\n",
        "\n",
        "def fitness_function_ssim(individual):\n",
        "\n",
        "  #basic score-> compare similarity between generated image and reference image\n",
        "  #similarity determined by Structural Similarity Index which considers changes in structural information, luminance, and contrast\n",
        "  # ssim value ranges from -1 to 1\n",
        "\n",
        "  #removing the batch dimension from generated image (diffusion model outputs 4 dim batch-first, channel-last image)\n",
        "  g_image=individual[\"image\"]\n",
        "  gen_image=g_image[0]\n",
        "  ssim_index, _ = ssim(gen_image, reference_image_np, full=True,win_size=7, channel_axis=2)\n",
        "  #print(f'SSIM: {ssim_index}')\n",
        "\n",
        "  return ssim_index\n",
        "\n",
        "'''\n",
        "# testing out ssim\n",
        "image_1_without_channel=image_1[0]\n",
        "ssim_index1, _ = ssim(image_1_without_channel, reference_image_np, full=True, win_size=7, channel_axis=2)\n",
        "print(\"Fitness of generated image 1, relative to actual image: \", ssim_index1)\n",
        "\n",
        "image_2_without_channel=image_2[0]\n",
        "\n",
        "ssim_index2, _ = ssim(image_2_without_channel, reference_image_np, full=True,win_size=7,channel_axis=2)\n",
        "print(\"Fitness of generated image 2, relative to actual image: \",ssim_index2)'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OU1R4eF7tqEK"
      },
      "source": [
        "### Defining the CLIP based fitness function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QUZogU-juHWh"
      },
      "outputs": [],
      "source": [
        "clip_model, preprocess = clip.load(\"ViT-B/32\", device=DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mYc6D8DrNaug"
      },
      "outputs": [],
      "source": [
        "# Loading the summarization pipeline that is to be used to summarise descriptions if they're over 77 tokens\n",
        "# this is required because CLIP only accepts a sequence of 77 tokens as input.\n",
        "#summarizer = pipeline(\"summarization\")\n",
        "\n",
        "#function that takes in a individual description/prompt and outputs a summarised version.\n",
        "def summarize_description(description,max_length):\n",
        "  #sentence=\"Vibrant boats an orange beach a vivid blue sky their bold colors the ocean.\"\n",
        "  prompt = f\"Summarize this sentence: {description}. The summary should have a maximum of {max_length} tokens. Maintain the tone and as much detail as in original sentence as possible\"\n",
        "  messages = [\n",
        "      {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "      {\"role\": \"user\", \"content\": prompt}\n",
        "  ]\n",
        "\n",
        "  generation_args = {\n",
        "      \"max_new_tokens\": 50,\n",
        "      \"return_full_text\": False,\n",
        "      \"temperature\": 0.0,\n",
        "      \"do_sample\": False,\n",
        "  }\n",
        "\n",
        "  output = pipe(messages, **generation_args)\n",
        "\n",
        "  return output[0]['generated_text']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y8nQGk4euByF"
      },
      "outputs": [],
      "source": [
        "#image converted to a tensor\n",
        "def preprocess_image(image):\n",
        "  image = preprocess(image).unsqueeze(0).to(DEVICE)\n",
        "  #print(\"Pre-processed image type:\",type(image))\n",
        "  #print(\"pre-processed image dimensions,\",image.shape)\n",
        "  return image\n",
        "\n",
        "def preprocess_caption(caption):\n",
        "  #if the prompt/caption/description is more than 77 tokens long, pass it into the summarizer to get a shorter summary for CLIP to work on\n",
        "\n",
        "  if len(caption.split(' ')) > 50:\n",
        "    print(\"Description too long for clip, being summarized\")\n",
        "    summ_caption = summarize_description(caption,max_length=50)\n",
        "    token = clip.tokenize([summ_caption]).to(DEVICE)\n",
        "    return token,summ_caption\n",
        "\n",
        "  else:\n",
        "    token = clip.tokenize([caption]).to(DEVICE)\n",
        "    return token,caption\n",
        "\n",
        "def get_image_embedding(image):\n",
        "    with torch.no_grad():\n",
        "        image_features = clip_model.encode_image(image)\n",
        "    return image_features\n",
        "\n",
        "def get_text_embedding(caption):\n",
        "    with torch.no_grad():\n",
        "        text_features = clip_model.encode_text(caption)\n",
        "    return text_features\n",
        "\n",
        "img=preprocess_image(reference_image_pil)\n",
        "print(img.type)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "CAPTION=\"The picture features four distinct boats, each with unique color patterns, against an orange shoreline. The visual elements include dynamic shapes that create a balanced composition, capturing the viewer's eye. The bold orange sands contrast with the fiery orange hues, while the shadows cast by the bamboo masts add depth to the sand and masts. Each boat is set against a deep blueish sky, which serves as the background.\"\n",
        "res=preprocess_caption(CAPTION)\n",
        "tok=res[0]\n",
        "cap=res[1]\n",
        "print(cap)\n",
        "print(tok.shape[1])\n"
      ],
      "metadata": {
        "id": "RJHf1N3V92ul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CAPTION2= \"The scene presents a tableau of four boats, their ornate curves and bold hues harmonizing with the fiery orange sands. Shadows stretch from bamboo masts, casting a playful dance against the azure horizon.\"\n",
        "print(summarize_description(CAPTION2, max_length=50))"
      ],
      "metadata": {
        "id": "US92lAwbFzH9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BQnX-ubZu-pv"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def compute_similarity(image_embedding, text_embedding):\n",
        "    # Normalize the embeddings\n",
        "    image_embedding = F.normalize(image_embedding, p=2, dim=-1)\n",
        "    text_embedding = F.normalize(text_embedding, p=2, dim=-1)\n",
        "\n",
        "    # Compute cosine similarity\n",
        "    similarity = torch.matmul(text_embedding, image_embedding.T).item()\n",
        "    return similarity\n",
        "\n",
        "\n",
        "def fitness_function_clip(individual):\n",
        "  individual_prompt=individual[\"prompt\"]\n",
        "  ref_image=preprocess_image(reference_image_pil)\n",
        "\n",
        "  preprocess_caption_response = preprocess_caption(individual_prompt)\n",
        "\n",
        "  token=preprocess_caption_response[0]\n",
        "  individual[\"prompt\"]=preprocess_caption_response[1]\n",
        "\n",
        "  # Get embeddings\n",
        "  image_embedding = get_image_embedding(ref_image)\n",
        "  text_embedding = get_text_embedding(token)\n",
        "\n",
        "  # Compute similarity\n",
        "  fitness_score = compute_similarity(image_embedding, text_embedding)\n",
        "\n",
        "  return fitness_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "POjef0sTTdBX"
      },
      "source": [
        "### Defining the mutation function(s)\n",
        "\n",
        "Synonym Mutate replaces the words that hit the mutation chance with a semantically similar synonym.\n",
        "\n",
        "Vocab Mutate appends a descriptive word like \"Water colors, Van Gogh, to the description being mutated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xx8gjG_QTgR9"
      },
      "outputs": [],
      "source": [
        "import random as rd\n",
        "# The MUTATION operator\n",
        "\n",
        "def synonym_mutate(individual, mutation_rate=0.3):\n",
        "    print(\"Performing Synonym mutatation\")\n",
        "    sentence=individual[\"prompt\"]\n",
        "    words = word_tokenize(sentence)\n",
        "    pos_tags = pos_tag(words)\n",
        "    for i in range(len(words)):\n",
        "        word, pos = pos_tags[i]\n",
        "        if word.lower() not in stop_words and pos in  ['JJ', 'JJR', 'JJS', 'NN', 'NNS', 'NNP', 'NNPS', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']:  # Adjectives, Verbs and Nouns\n",
        "            if random.random() < mutation_rate:\n",
        "                synonyms = get_synonyms(word)\n",
        "                if synonyms:\n",
        "                    words[i] = random.choice(synonyms)\n",
        "    #only return the mutated sentence if it is semantically similar to version pre-mutation (could help avoid boat becoming gravy holder and stuff like that )\n",
        "    if is_sentence_semantically_similar(sentence,' '.join(words)):\n",
        "      #print(' '.join(words)+\" is similar to \"+ sentence)\n",
        "      return ' '.join(words)\n",
        "\n",
        "    return sentence\n",
        "\n",
        "def vocab_mutate(individual,vocab,mutation_rate=0.3):\n",
        "  sentence=individual[\"prompt\"]\n",
        "  words=word_tokenize(sentence)\n",
        "\n",
        "  if random.random()< mutation_rate:\n",
        "    # Loop until a new vocab word is found\n",
        "    print(\"Performing Vocab mutatation\")\n",
        "    while True:\n",
        "        vocab_word = random.choice(vocab)\n",
        "        if vocab_word not in words:\n",
        "            words.append(vocab_word)\n",
        "            break  # Exit the loop once a new word is added\n",
        "\n",
        "  return ' '.join(words)\n",
        "\n",
        "'''\n",
        "\n",
        "# trialling out both the synonym and context based mutations\n",
        "sentence = \"Vibrant boats rest on an orange beach under a vivid blue sky, their bold colors creating a striking contrast with the natural surroundings.\"\n",
        "\n",
        "i1={\"prompt\":sentence}\n",
        "mutated_sentence1 = synonym_mutate(i1)\n",
        "print(f'Mutated Sentence 1 [Synonym based]: {mutated_sentence1}')\n",
        "#print(f'Mutated Sentence 2 [Contextual]: {mutated_sentence2}')\n",
        "#mutated_sentence2 = contextual_mutation(i1, vocab)\n",
        "'''\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tLPdeZ6dktIv"
      },
      "outputs": [],
      "source": [
        "#this run has a 100% chance of vocab mutate  (if random.random()<0.0)\n",
        "#this run has a 100% chance of synonym mutate (if random.random()>=0.0)\n",
        "\n",
        "def mutate_population(population,mutation_rate,vocab):\n",
        "  mutated_population=[]\n",
        "\n",
        "  for individual in population:\n",
        "    if random.random()>=0.0: #chance of synonym mutate\n",
        "      individual[\"prompt\"]= synonym_mutate(individual,mutation_rate)\n",
        "      mutated_population.append(individual)\n",
        "\n",
        "    else:#chance of vocab mutate\n",
        "      individual[\"prompt\"]=vocab_mutate(individual,vocab,mutation_rate)\n",
        "      mutated_population.append(individual)\n",
        "\n",
        "  return mutated_population\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_0qTxT4MC9M"
      },
      "source": [
        "### Defining the crossover function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JZojzptJlFal"
      },
      "outputs": [],
      "source": [
        "#phrase based crossover\n",
        "def phrase_based_crossover(parent1, parent2,use_llm_api=False):\n",
        "  if use_llm_api:\n",
        "    correction_function=correct_sentences_api\n",
        "  else:\n",
        "      correction_function=correct_sentences\n",
        "  parent1_phrases = extract_phrases(parent1[\"prompt\"])\n",
        "  parent2_phrases = extract_phrases(parent2[\"prompt\"])\n",
        "\n",
        "  if len(parent1_phrases) > 1 and len(parent2_phrases) > 1:\n",
        "      crossover_point1 = random.randint(1, len(parent1_phrases) - 1)\n",
        "      crossover_point2 = random.randint(1, len(parent2_phrases) - 1)\n",
        "\n",
        "      child1_phrases = parent1_phrases[:crossover_point1] + parent2_phrases[crossover_point2:]\n",
        "      child2_phrases = parent2_phrases[:crossover_point2] + parent1_phrases[crossover_point1:]\n",
        "\n",
        "      #ensure that the children are correct sentences/descriptions\n",
        "      corrected_child_phrases=correction_function([' '.join(child1_phrases),' '.join(child2_phrases)])\n",
        "\n",
        "      child1_corrected_phrase=corrected_child_phrases[0]\n",
        "      child2_corrected_phrase=corrected_child_phrases[1]\n",
        "\n",
        "      return child1_corrected_phrase, child2_corrected_phrase\n",
        "  else:\n",
        "      return parent1, parent2\n",
        "\n",
        "# the cross over that seems more useful-> workflow involves cross-over + llm based correction\n",
        "def noun_phrase_based_crossover(parent1, parent2,use_llm_api=False):\n",
        "    #print(\"Crossing over parents using their noun phrases\")\n",
        "    if use_llm_api:\n",
        "      correction_function=correct_sentences_api\n",
        "    else:\n",
        "      correction_function=correct_sentences\n",
        "\n",
        "    parent1_phrases = extract_noun_phrases_spacy(parent1[\"prompt\"])\n",
        "    parent2_phrases = extract_noun_phrases_spacy(parent2[\"prompt\"])\n",
        "\n",
        "    if len(parent1_phrases) > 1 and len(parent2_phrases) > 1:\n",
        "        crossover_point1 = random.randint(1, len(parent1_phrases) - 1)\n",
        "        crossover_point2 = random.randint(1, len(parent2_phrases) - 1)\n",
        "\n",
        "        child1_phrases = parent1_phrases[:crossover_point1] + parent2_phrases[crossover_point2:]\n",
        "        child2_phrases = parent2_phrases[:crossover_point2] + parent1_phrases[crossover_point1:]\n",
        "\n",
        "        #print(\"Child 1 before being corrected: \",' '.join(child1_phrases))\n",
        "        #print(\"Child 2 before being corrected: \",' '.join(child2_phrases))\n",
        "\n",
        "        #ensure that the children are correct sentences/descriptions\n",
        "        #print(\"Correcting the sentences\")\n",
        "        corrected_child_phrases=correction_function([' '.join(child1_phrases),' '.join(child2_phrases)])\n",
        "\n",
        "        child1_corrected_phrase=corrected_child_phrases[0]\n",
        "        child2_corrected_phrase=corrected_child_phrases[1]\n",
        "\n",
        "        #print(\"Child 1 AFTER being corrected: \",child1_corrected_phrase)\n",
        "        #print(\"Child 2 AFTER being corrected: \",child2_corrected_phrase)\n",
        "\n",
        "        return child1_corrected_phrase, child2_corrected_phrase\n",
        "    else:\n",
        "        return parent1[\"prompt\"], parent2[\"prompt\"]\n",
        "\n",
        "\n",
        "\n",
        "# TO DO: Phrase based crossover can be fixed length (first one is what we're doing and another one can be \"adaptive\" n-word number- as n grows larger, allow for more phrases to be crossed over )\n",
        "trial_prompt_1=\"Four colorful boats are on a beach next to the ocean.\"\n",
        "trial_prompt_2=\"Vibrant boats rest on an orange beach under a vivid blue sky, their bold colors creating a striking contrast with the natural surroundings.\"\n",
        "\n",
        "parent1 = {\"prompt\":trial_prompt_1}\n",
        "parent2 = {\"prompt\":trial_prompt_2}\n",
        "\n",
        "'''#trialling out the phrase based cross-over\n",
        "child1, child2 = phrase_based_crossover(parent1, parent2, use_llm_api=False)\n",
        "print(f'Phrase based Child 1: {child1}')\n",
        "print(f'Phrase based Child 2: {child2}')'''\n",
        "\n",
        "print(\"\\n\\n\")\n",
        "#trialling out the phrase based cross-over\n",
        "nchild1, nchild2 = noun_phrase_based_crossover(parent1, parent2,use_llm_api=False)\n",
        "print(f'noun Phrase based Child 1: {nchild1}')\n",
        "print(f'noun Phrase based Child 2: {nchild2}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ko4yJGcZTjN0"
      },
      "outputs": [],
      "source": [
        "# The CROSSOVER operator -generates kids until population size is back to normal (after selection process)\n",
        "def crossover_population(population,cross_over_func,num_parents=2,POP_SIZE=20):\n",
        "\n",
        "  needed_kids=POP_SIZE-len(population)\n",
        "  print(f\"Crossover needs to create {needed_kids} new samples\")\n",
        "  children=[]\n",
        "\n",
        "  #using the specified cross over function\n",
        "  if cross_over_func==\"NOUN\":\n",
        "    cross_over_func=noun_phrase_based_crossover\n",
        "  elif cross_over_func==\"PHRASE\":\n",
        "    cross_over_func=phrase_based_crossover\n",
        "\n",
        "  while len(children)<needed_kids:\n",
        "    #choosing 2 parents from the population\n",
        "    parent1,parent2=rd.sample(population,2)\n",
        "    #actually doing the cross over\n",
        "    child1prompt,child2prompt=cross_over_func(parent1, parent2,use_llm_api=False)\n",
        "\n",
        "\n",
        "    if len(children) < needed_kids:\n",
        "      child1caption=\"\"\n",
        "      if len(child1prompt.split(' ')) > 50:\n",
        "        print(\"Child 1 description too long for clip, being summarized\")\n",
        "        child1caption = summarize_description(child1prompt,max_length=50)\n",
        "\n",
        "      else:\n",
        "        child1caption=child1prompt\n",
        "\n",
        "      child1 = {\n",
        "          \"prompt\": child1caption,\n",
        "          \"image\": None,\n",
        "          \"fitness\": -1000\n",
        "      }\n",
        "      child1[\"fitness\"]=round(fitness_function_clip(child1),3)\n",
        "      children.append(child1)\n",
        "\n",
        "    if len(children) < needed_kids:\n",
        "      child2caption=\"\"\n",
        "      if len(child2prompt.split(' ')) > 50:\n",
        "        print(\"Child 2 description too long for clip, being summarized\")\n",
        "        child2caption = summarize_description(child2prompt,max_length=50)\n",
        "\n",
        "      else:\n",
        "        child2caption=child2prompt\n",
        "      child2 = {\n",
        "          \"prompt\": child2prompt,\n",
        "          \"image\": None,\n",
        "          \"fitness\": -1000\n",
        "      }\n",
        "      child2[\"fitness\"]=round(fitness_function_clip(child2),3)\n",
        "      children.append(child2)\n",
        "\n",
        "  return children"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vDjMIzXVxjRv"
      },
      "source": [
        "### Tournament Selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5UfxD851mxTW"
      },
      "outputs": [],
      "source": [
        "#fight function and tournament selection function\n",
        "def fight(fight_participants):\n",
        "  currWinner = fight_participants[0]\n",
        "    #loop through participants and identify individual with highest fitness\n",
        "  for i in range(1, len(fight_participants)):\n",
        "      if fight_participants[i][\"fitness\"] > currWinner[\"fitness\"]:\n",
        "          currWinner = fight_participants[i]\n",
        "  return currWinner\n",
        "\n",
        "# Tournament selection function should return winners of the tournament\n",
        "def tournament_selection(population, tournament_size,reference_image=reference_image):\n",
        "    strongest_individuals=[]\n",
        "\n",
        "    #making sure every text prompt is involved in the fight\n",
        "    num_rounds=round(len(population)/tournament_size)\n",
        "    for i in range(0, num_rounds):\n",
        "\n",
        "        #giving some prompts a pass or a by if a fight doesn't have enough participants\n",
        "        if (len(population)<tournament_size):\n",
        "            for p in population:\n",
        "                strongest_individuals.append(p)\n",
        "            return strongest_individuals\n",
        "\n",
        "\n",
        "        #randomly choosing tournament_size fighters to fight\n",
        "        individuals_to_fight=rd.sample(population,tournament_size)\n",
        "\n",
        "        #getting the winner of the fight and adding him to winners list\n",
        "        strongest_individuals.append(fight(individuals_to_fight))\n",
        "\n",
        "        #remove combatants from population (a prompt that has already \"fought\" won't fight again)\n",
        "        for fighter in individuals_to_fight:\n",
        "            population.remove(fighter)\n",
        "\n",
        "    return strongest_individuals\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ajOVecBS_Ft"
      },
      "source": [
        "### Defining the basic parameters of the Genetic Algorithm's evolutionary loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HLObLYM6zFNs"
      },
      "outputs": [],
      "source": [
        "TOTAL_GENERATIONS=50\n",
        "TOURNAMENT_SIZE=2\n",
        "NUM_PARENTS=2\n",
        "MUTATION_RATE=0.3\n",
        "POP_SIZE=10\n",
        "CROSSOVER_FUNC=\"NOUN\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6nxbQWzgTN17"
      },
      "source": [
        "### Initialising the population of prompts and the image they should ideally generate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wQ3mVA3y6DZr"
      },
      "outputs": [],
      "source": [
        "def read_prompts_from_file(file_path,pop_size):\n",
        "    with open(file_path, 'r') as file:\n",
        "        prompts = [line.strip() for line in file if line.strip()]\n",
        "\n",
        "    prompts_list=[]\n",
        "\n",
        "    for idx,prompt in enumerate(prompts):\n",
        "      if idx==pop_size:\n",
        "        break\n",
        "      print(f\"prompt {idx}: \"+ prompt)\n",
        "      prompts_list.append(prompt.strip())\n",
        "\n",
        "    return prompts_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "piZrHsfAbflZ"
      },
      "outputs": [],
      "source": [
        "def process_json(api_response):\n",
        "  # Use the regex to find all matches\n",
        "  matches = re.findall(r'\\d+\\.\\s\\*\\*[^:]+:\\*\\*\\s(.+?)(?=\\n\\d+\\.|\\Z)', api_response, re.DOTALL)\n",
        "\n",
        "  # Print the results\n",
        "  for match in matches:\n",
        "      print(match.strip())\n",
        "\n",
        "  return matches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A2TE1octTTy5"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import json\n",
        "#initially population initialisation can be just reading in the text file of pre-defined prompts\n",
        "def initialise_population(pop_size=20,image_path=None, read_from_file=False,make_api_call=True):\n",
        "  print(\"Initialising population\")\n",
        "  #population is going to be a list of dict objects\n",
        "  # population=[individual1:{\"prompt\":\"a description of the image\",\"fitness\":fitness score, \"image\":generatedimage}, individual2]\n",
        "  initial_population=[]\n",
        "\n",
        "  print(\"Obtaining prompts\")\n",
        "\n",
        "  if read_from_file:\n",
        "    # Reading in the prompts from text file (these will make up the initial population)\n",
        "    prompts = read_prompts_from_file(f\"/content/Cafe-Experiment3-Data.txt\",pop_size)\n",
        "\n",
        "    print(f\" Read in {len(prompts)} prompts\")\n",
        "    print(\"Creating representations\")\n",
        "    #initially setting fitness to a very low value for every prompt/text description\n",
        "    for prompt in prompts:\n",
        "      individual={}\n",
        "      individual[\"prompt\"]=prompt\n",
        "      #individual[\"image\"]=generate_image(individual)\n",
        "      individual[\"fitness\"]=-1000\n",
        "\n",
        "      initial_population.append(individual)\n",
        "\n",
        "#else use the LLM available via API to get initial descriptions\n",
        "  elif make_api_call:\n",
        "    print(\"Using the LLM to get initial descriptions\")\n",
        "\n",
        "    descriptions_response=get_initial_descriptions(path_to_image=image_path,num_descriptions=pop_size)\n",
        "    desc_resp_json=descriptions_response.json()\n",
        "    #desc_resp_json[\"choices\"][0]['message']['content']\n",
        "\n",
        "    file_name = 'descriptions_response.json'\n",
        "\n",
        "    # Open the file in write mode and dump the JSON data\n",
        "    with open(file_name, 'w') as file:\n",
        "      json.dump(desc_resp_json, file)\n",
        "\n",
        "    print(f\"JSON data has been written to {file_name}\")\n",
        "\n",
        "    # Find all descriptions\n",
        "    descriptions = process_json(desc_resp_json[\"choices\"][0]['message']['content'])#desc_resp_json[\"choices\"][0]['message']['content']\n",
        "    print(\"Number of descriptions:\",len(descriptions))\n",
        "\n",
        "    print(descriptions)\n",
        "\n",
        "    for prompt in descriptions:\n",
        "      individual={}\n",
        "      individual[\"prompt\"]=prompt\n",
        "      #individual[\"image\"]=generate_image(individual)\n",
        "      individual[\"fitness\"]=-1000\n",
        "      initial_population.append(individual)\n",
        "\n",
        "  else: #if api call has been made already\n",
        "\n",
        "    print(\"Using LLM api call response stored in file\")\n",
        "    # Find all descriptions\n",
        "    with open(\"descriptions_response.json\", 'r') as file:\n",
        "      data = json.load(file)\n",
        "    print(data['choices'][0]['message']['content'])\n",
        "    descriptions = process_json(data[\"choices\"][0]['message']['content'])#desc_resp_json[\"choices\"][0]['message']['content']\n",
        "    print(\"Number of descriptions:\",len(descriptions))\n",
        "\n",
        "    print(descriptions)\n",
        "\n",
        "    for prompt in descriptions:\n",
        "      individual={}\n",
        "      individual[\"prompt\"]=prompt\n",
        "      #individual[\"image\"]=generate_image(individual)#Stable Diffusion used to create image from description\n",
        "      individual[\"fitness\"]=-1000 #fitness score arbitrarily set to a value initially\n",
        "      initial_population.append(individual)\n",
        "\n",
        "\n",
        "  return initial_population\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''individual={}\n",
        "individual[\"prompt\"]=\"Insert description of image here\"\n",
        "#Stable Diffusion used to create image from description/prompt\n",
        "individual[\"image\"]=generate_image(individual)\n",
        "individual[\"fitness\"]=0.18 #fitness score'''"
      ],
      "metadata": {
        "id": "Ji7CAXa9bGPq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6yg1fOB9Z2dc"
      },
      "source": [
        "### Function to get descriptive words about an image to be used in Vocab mutate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d4PakNMlZ5yS"
      },
      "outputs": [],
      "source": [
        "def get_descriptive_words_api(path_to_image, num_words):\n",
        "  BASE_PROMPT= f\"Generate {num_words} descriptive words or phrases for this image.\"\n",
        "  get_descriptives_prompt= f\"Could you give me {num_words} words that are descriptive of this image?. Examples of such words are painting, watercolours\"\n",
        "  image_path = path_to_image\n",
        "\n",
        "  # Getting the base64 string\n",
        "  base64_image = encode_image(image_path)\n",
        "\n",
        "  headers = {\n",
        "    \"Content-Type\": \"application/json\",\n",
        "    \"Authorization\": f\"Bearer {api_key}\"\n",
        "  }\n",
        "\n",
        "  payload = {\n",
        "    \"model\": \"gpt-4o-mini\",\n",
        "    \"messages\": [\n",
        "      {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "          {\n",
        "            \"type\": \"text\",\n",
        "            \"text\": BASE_PROMPT\n",
        "          },\n",
        "          {\n",
        "            \"type\": \"image_url\",\n",
        "            \"image_url\": {\n",
        "              \"url\": f\"data:image/jpeg;base64,{base64_image}\"\n",
        "            }\n",
        "          }\n",
        "        ]\n",
        "      }\n",
        "    ],\n",
        "    \"max_tokens\": 500\n",
        "  }\n",
        "\n",
        "  response = requests.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload)\n",
        "\n",
        "  print(response.json())\n",
        "\n",
        "  resp_json=response.json()\n",
        "  resp=resp_json[\"choices\"][0]['message']['content']\n",
        "\n",
        "  # Split the text into individual lines\n",
        "  lines = resp.strip().split(\"\\n\")\n",
        "\n",
        "  # Extract descriptive words/phrases, ignoring the first line (greeting line)\n",
        "  descriptive_list = [line.split(\". \", 1)[1] for line in lines[1:] if \". \" in line]\n",
        "\n",
        "  # Print the list\n",
        "  print(descriptive_list)\n",
        "\n",
        "  return descriptive_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNCni_FtxCBf"
      },
      "source": [
        "### Statistical stuff and function to evaluate fitness of population"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PPMXktC4wStH"
      },
      "outputs": [],
      "source": [
        "#function that loops through population and evaluates/assigns fitness for each individual\n",
        "def evaluate_population_fitness(population,use_ssim=False):\n",
        "\n",
        "  fitness_population=population\n",
        "  for index,individual in enumerate(fitness_population):\n",
        "    #every x generations the existing fitness of the individual is multiplied by the ssim score\n",
        "    if use_ssim:\n",
        "      individual[\"fitness\"]*=(1+round(fitness_function_ssim(individual),3))\n",
        "    #normally (for every generation) the fitness of an individual is calculated using clip score.\n",
        "    else:\n",
        "      individual[\"fitness\"]=round(fitness_function_clip(individual),3)\n",
        "\n",
        "  return fitness_population"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XzrPFS-YhjcN"
      },
      "outputs": [],
      "source": [
        "def get_population_average_fitness(population):\n",
        "  total_fitness=0\n",
        "  for individual in population:\n",
        "    #print(f\"Adding {individual['fitness']} to {total_fitness}\")\n",
        "    total_fitness+=individual[\"fitness\"]\n",
        "\n",
        "  #print(f\"Dividing {total_fitness} by {len(population)}\")\n",
        "  return total_fitness/len(population)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "clpXJ86thtmm"
      },
      "outputs": [],
      "source": [
        "def get_population_max_fitness(population):\n",
        "  max_fitness=0\n",
        "  fittest_description=None\n",
        "  for individual in population:\n",
        "    if individual[\"fitness\"]>max_fitness:\n",
        "      max_fitness=individual[\"fitness\"]\n",
        "      fittest_description=individual[\"prompt\"]\n",
        "\n",
        "  return max_fitness,fittest_description"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zDvCt-bO7cFF"
      },
      "outputs": [],
      "source": [
        "def print_top_individuals(population, top_n=5):\n",
        "    # Sort the population by fitness in descending order\n",
        "    sorted_population = sorted(population, key=lambda x: x[\"fitness\"], reverse=True)\n",
        "    top_individuals = sorted_population[:top_n]\n",
        "    print(f\"Top {top_n} individuals:\")\n",
        "    for idx, individual in enumerate(top_individuals):\n",
        "        print(f\"Rank {idx+1}: Fitness = {individual['fitness']}, Prompt = {individual['prompt']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIJbWScMw9Px"
      },
      "source": [
        "### Running the Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nOyLJwUPGiUi"
      },
      "outputs": [],
      "source": [
        "#initialising the population\n",
        "IMAGE_PATH=\"/content/VincentVanGogh/Watercolors/The Night Cafe in Arles.jpg\"\n",
        "initial_population=initialise_population(pop_size=10,image_path=IMAGE_PATH,read_from_file=True,make_api_call=False)\n",
        "#Fitness still needs to be calculated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mgX_vG3NY0vL"
      },
      "outputs": [],
      "source": [
        "print(len(initial_population))\n",
        "\n",
        "#generating the vocabulary of descriptive words\n",
        "num_words=30\n",
        "vocab_list=get_descriptive_words_api(IMAGE_PATH, num_words)\n",
        "\n",
        "vocab_list.append(\"watercolours painting\")\n",
        "vocab_list.append(\"Van Gogh style\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "42NpVKloT5Jf"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "population = copy.deepcopy(initial_population)\n",
        "\n",
        "avg_fitness_per_generation=[]\n",
        "max_fitness_per_generation=[]\n",
        "best_individual_per_generation=[]\n",
        "\n",
        "MUTATION_RATE=0.3\n",
        "\n",
        "#Evolutionary loop\n",
        "for generation in range(1,51):\n",
        "\n",
        "  #evaluate fitness of entire population\n",
        "  population=evaluate_population_fitness(population=population,use_ssim=False)\n",
        "  print(f\"\\n---------------------------------------Generation {generation}---------------------------------------\")\n",
        "  print_top_individuals(population, top_n=3)\n",
        "\n",
        "  # TO DO: have an elitism toggle to save a % high fitness individuals every generation to guarantee they make it to the next generation\n",
        "  #tournament selection\n",
        "  print(\"\\nTOURNAMENT SELECTION:\")\n",
        "  print(\"Performing tournament selection on the population\")\n",
        "  print(\"Number of prompts in population: \",len(population))\n",
        "  winners=tournament_selection(population=population,tournament_size=TOURNAMENT_SIZE)\n",
        "  population=winners\n",
        "\n",
        "  print(\"Number of prompts in population (After tournament): \",len(population))\n",
        "\n",
        "  #crossover between winners to generate children which are then added to the population\n",
        "  print(\"\\nCROSSOVER PHASE:\")\n",
        "  print(\"Performing Cross-over on the population\")\n",
        "  crossed_over_children=crossover_population(population=population,num_parents=NUM_PARENTS,POP_SIZE=len(initial_population),cross_over_func=CROSSOVER_FUNC) #giving the function the population and the number of parents involved in a crossover\n",
        "  population.extend(crossed_over_children)\n",
        "  print(\"Number of prompts in population (After crossover): \",len(population))\n",
        "  print(\"Cross-over phase completed\")\n",
        "\n",
        "  #mutation\n",
        "  print(\"\\nMUTATION PHASE:\")\n",
        "  print(\"Mutating population\")\n",
        "  population=mutate_population(population=population,mutation_rate=MUTATION_RATE,vocab=vocab_list)\n",
        "  print(\"Mutation phase completed\")\n",
        "\n",
        "  #every 10 generations in, use the ssim score as a multiplier to the clip fitness\n",
        "  #generate \"new\" images for current individual prompts in population\n",
        "\n",
        "\n",
        "  if generation % 10 ==0:\n",
        "    print(\"\\nIMAGE GENERATION PHASE:\")\n",
        "    print(\"Generating images for prompts in population\")\n",
        "    for individual in population:\n",
        "      individual[\"image\"]=generate_image(individual)\n",
        "    population=evaluate_population_fitness(population=population,use_ssim=True)\n",
        "\n",
        "  #storing the average fitness per generation and also storing the max fitness per generation (along with the best description per generation)\n",
        "  avg_fitness=get_population_average_fitness(population)\n",
        "  avg_fitness_per_generation.append(avg_fitness)\n",
        "\n",
        "  max_fitness_vals=get_population_max_fitness(population)\n",
        "  best_individual=max_fitness_vals[1]\n",
        "  best_individual_per_generation.append(best_individual)\n",
        "  max_fitness_per_generation.append(max_fitness_vals[0])\n",
        "\n",
        "  print(f\"-------------------Generation {generation} Results summary:---------------------------\")\n",
        "  print(f\"Average Fitness in generation {generation}: {avg_fitness}\" )\n",
        "  print(f\"Best Individual (fitness:{max_fitness_vals[0]}): {best_individual}\")\n",
        "  print(f\"\\nEND OF GENERATION {generation}\")\n",
        "\n",
        "  #end of generation\n",
        "\n",
        "#end of evolutionary loop\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4P6viu6HDaL5"
      },
      "outputs": [],
      "source": [
        "#final fitness evaluation after final selection + cross-over/mutation\n",
        "population=evaluate_population_fitness(population)\n",
        "print_top_individuals(population,5)\n",
        "avg_fitness=get_population_average_fitness(population)\n",
        "avg_fitness_per_generation.append(avg_fitness)\n",
        "\n",
        "max_fitness_vals=get_population_max_fitness(population)\n",
        "best_individual_per_generation=max_fitness_vals[1]\n",
        "max_fitness_per_generation.append(max_fitness_vals[0])\n",
        "\n",
        "print(f\"-------------------Generation {generation} Results summary:---------------------------\")\n",
        "print(f\"Average Fitness in generation {generation}: {avg_fitness}\" )\n",
        "print(f\"Best Individual (fitness:{max_fitness_vals[0]}): {best_individual}\")\n",
        "print(f\"\\nEND OF GENERATION {generation}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQkRRpKfUwsu"
      },
      "source": [
        "### Writing results to a file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K7J6L0E8UvJc"
      },
      "outputs": [],
      "source": [
        "def write_fitness_to_file(file_path, average_fitness, max_fitness):\n",
        "    \"\"\"\n",
        "    Writes the average fitness and max fitness per generation to a file.\n",
        "\n",
        "    Parameters:\n",
        "    - file_path: Path to the file where the data will be written.\n",
        "    - average_fitness: List of average fitness values per generation.\n",
        "    - max_fitness: List of max fitness values per generation.\n",
        "    \"\"\"\n",
        "    with open(file_path, 'w') as file:\n",
        "        file.write(\"Generation,Average Fitness,Max Fitness\\n\")\n",
        "        for i in range(len(average_fitness)):\n",
        "          file.write(f\"{i},{average_fitness[i]},{max_fitness[i]}\\n\")\n",
        "\n",
        "file_path = f\"/content/Experiment2-SynonymMutate-population{len(population)}_fitness_per_generation.csv\"\n",
        "write_fitness_to_file(file_path, avg_fitness_per_generation, max_fitness_per_generation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGvp5n4DR_G_"
      },
      "source": [
        "### Graphs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qBs-JDaQSAjO"
      },
      "outputs": [],
      "source": [
        "def plot_fitness(x, y, xlabel, ylabel, title):\n",
        "    \"\"\"\n",
        "    Plots a graph given x and y data along with axis labels and a title.\n",
        "\n",
        "    Parameters:\n",
        "    - x: List of values for the x-axis\n",
        "    - y: List of values for the y-axis\n",
        "    - xlabel: Label for the x-axis\n",
        "    - ylabel: Label for the y-axis\n",
        "    - title: Title of the plot\n",
        "    \"\"\"\n",
        "    #multiplying the fitness values by 100 to make for better visualisation\n",
        "    transformed_y = [fitness * 100 for fitness in y]\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(x, transformed_y, marker='o', linestyle='-', color='b')\n",
        "    plt.xlabel(xlabel)\n",
        "    plt.ylabel(ylabel)\n",
        "    plt.title(title)\n",
        "    plt.grid(True)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uzbQHLMKSCG2"
      },
      "outputs": [],
      "source": [
        "print(\"Plotting Average fitness against generations\")\n",
        "plot_fitness(x=range(len(avg_fitness_per_generation)), y=avg_fitness_per_generation, xlabel='Generation', ylabel='Average Fitness', title=f\"Average Fitness per Generation [Population:{len(population)}]\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LRiaQnOdSNvD"
      },
      "outputs": [],
      "source": [
        "print(\"Plotting Maximum fitness against generations\")\n",
        "plot_fitness(x=range(len(max_fitness_per_generation)),y=max_fitness_per_generation,xlabel='Generation',ylabel='Max. Fitness',title=f\"Maximum Fitness per Generation [Population:{len(population)}]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "htb4ikqvTHE-"
      },
      "source": [
        "#Experimental stuff /Old code"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "BEST INDIVIDUAL (pop=10)-> \"\"The bold brushstrokes depict four boats on an orange sandy beach: an artistically designed fisherman's vessel, colorful boats, and an artistic composition.\" Fitness of 0.372 (CLIP score).\n",
        "\n",
        "Improved maximum fitness by 5% after 50 generations\n",
        "\n",
        "\n",
        "Experiment 2 (Vocab Mutate vs Synonym mutate)\n",
        "\"Fishing boats on the Beach\"\n",
        "\n",
        "Vocab Mutate:\n",
        "Best Individual (before correction)\" \"The painting depicts a cluster of four boats with varying colors on a sandy beach , with a vibrant blue sea in the background , and an artful representation of light and shadow . Bold outlines Colorful boats watercolours painting\" -> CLIP score of 0.356\n",
        "\n",
        "Best individual (after correction): \"\"The painting depicts a cluster of four boats in varying colors on a sandy beach, with a vibrant blue sea in the background, and an artful representation of light and shadow. The bold outlines and colorful boats are rendered in watercolors.\" -> CLIP score of 0.356\n",
        "\n",
        "Improved average fitness by 3% (31 to 34) and improved max fitness by 1.5% roughly.\n",
        "\n",
        "\n",
        "Synonym Mutate:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Experiment 3 (data collected from people)\n",
        "------------------------------------------------------------\n",
        "\"The Night Cafe in Arles\"\n",
        "\n",
        "Best individual:\n",
        "\"A Van Gogh painting depicts a caf with a pool table on the right side, bar stools, bold colors in the background, eccentric lighting, and tables with glassware. The walls have colorful accents.\"\" fitness of 0.412 (CLIP score)\n",
        "\n",
        "Improved maximum fitness by 3.5% to 4% (from 0.37 to 0.412) after 50 generations and average fitness by 7% at its peak(from 0.32 to 0.39)\n",
        "\n",
        "\n",
        "\"The Langlois Bridge at Arles\"\n",
        "\n",
        "Best Individual: \"\"A vibrant-colored wooden drawbridge on a windy hill, in the style of Vincent van Gogh, overlooks a flowing river on another windy hill, reflecting the historical context.\"\" - fitness of 0.393 (CLIP score)\n",
        "\n",
        "Improved maximum fitness by 4 to 5%  (from 0.34 to 0.39) after 50 generations and average fitness by 6% at its peak (from 0.31 to 0.36)"
      ],
      "metadata": {
        "id": "a4GxpxbSVEXb"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4XeNEyi0miED"
      },
      "source": [
        "#Old Code\n",
        "\n",
        "### Mechanism for cross-over (phrase based is fine):\n",
        "\n",
        "### If a better cross-over mechanism is found and implemented, it can happen x% of the time to direct the evolution. -> implemented noun phrase mutation as default, it doesn't happen x% of the time.\n",
        "\n",
        "### Mechanism to correct grammar after cross-over -> DONE\n",
        "\n",
        "### Mutation to add descriptive elements that were obtained initially (extra mutation) -> DONE\n",
        "\n",
        "### Mutation to remove a phrase randomly (Very low probability)\n",
        "\n",
        "### To show \"directed evolution\"-> implement elitism.\n",
        "\n",
        "\n",
        "2 Fitness functions: DONE\n",
        "\n",
        "1-> CLIP based (comparing text to reference image in the embedding space) used initially. DONE\n",
        "\n",
        "2-> Image based fitness function (SSIM) (every 10 generations) DONE\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7b-tC3o31lr"
      },
      "source": [
        "### old code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jTvaw7efQKMI"
      },
      "outputs": [],
      "source": [
        "'''!pip install transformers\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "\n",
        "model_name = \"openai/clip-vit-large-patch14-336\"\n",
        "processor = CLIPProcessor.from_pretrained(model_name)\n",
        "model = CLIPModel.from_pretrained(model_name)'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "goHJX5-5PWvZ"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "#\"Qwen/Qwen2-7B-Instruct\" stuff after tokenizer\n",
        "\n",
        "\n",
        "#sentence=\"Vibrant boats an orange beach a vivid blue sky their bold colors the ocean.\"\n",
        "prompt = f\"Correct this sentence: {sentence}\"\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": prompt}\n",
        "]\n",
        "\n",
        "text = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize=False,\n",
        "    add_generation_prompt=True\n",
        ")\n",
        "model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
        "\n",
        "generated_ids = model.generate(\n",
        "    model_inputs.input_ids,\n",
        "    max_new_tokens=512\n",
        ")\n",
        "generated_ids = [\n",
        "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
        "]\n",
        "\n",
        "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fgZfF_sT33Qu"
      },
      "outputs": [],
      "source": [
        "# Contextual mutation can be used in later stages of the experimentation process, its not ready yet.\n",
        "\n",
        "#TO DO: Vocabulary should be all words in the initial population of prompts (should I update vocabulary after every generation?).\n",
        "# OR should Vocabulary be obtained by providing an image to a multi-modal LLM and ask it to provide a list of adjectives that can describe the image (artistic, painting, watercolour and stuff like that).\n",
        "# Also for the replace bit-> should POS tag all words in prompt being mutated, POS tag vocabulary and then replace noun with noun, verb with verb etcetera.\n",
        "'''\n",
        "def contextual_mutation(individual, vocab, mutation_rate=0.3):\n",
        "    sentence=individual[\"prompt\"]\n",
        "    words = word_tokenize(sentence)\n",
        "    for i in range(len(words)):\n",
        "        if random.random() < mutation_rate:\n",
        "            #action = random.choice(['replace', 'add', 'remove'])\n",
        "            action=\"replace\"\n",
        "            if action == 'replace':\n",
        "                words[i] = random.choice(vocab)\n",
        "            elif action == 'add':\n",
        "                words.insert(i, random.choice(vocab))\n",
        "            elif action == 'remove' and len(words) > 1:\n",
        "                words.pop(i)\n",
        "    return ' '.join(words)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "6iAo23uBW-SZ",
        "wkmBS1bAaeJD",
        "NQ05IG3n9UdS",
        "JPvDUkWn9k8H",
        "r1i4qaFy9XOP",
        "If95y2mJ-t7N",
        "lxrABJsa_qd-",
        "vVupSNCY3cJP",
        "pMQL2eFP0yXU",
        "a6lf03woeMww",
        "b5h015nYeTah",
        "sz21Mgg4mjmV",
        "BDDoBjjuIhM6",
        "Mb4xATfjbBqm",
        "MelTDVW2ofUq",
        "quj8vdr-l9GT",
        "g_kn7T7xgp-c",
        "uSFdO_cVwkEw",
        "1BgMJeAyhxaJ",
        "Fy-SPTdTTX1c",
        "OU1R4eF7tqEK",
        "1ajOVecBS_Ft",
        "6nxbQWzgTN17",
        "6yg1fOB9Z2dc",
        "tNCni_FtxCBf",
        "htb4ikqvTHE-",
        "v7b-tC3o31lr"
      ],
      "gpuType": "T4",
      "provenance": [],
      "mount_file_id": "1f3fFkdvXPo8zGBc9J0u2k390zZW9YbAu",
      "authorship_tag": "ABX9TyN0DVsflR511M4KyPnEKas9",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}